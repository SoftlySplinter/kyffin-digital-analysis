\chapter{Implementation}

%The implementation should look at any issues you encountered as you tried to implement your design. During the work, you might have found that elements of your design were unnecessary or overly complex, perhaps third party libraries were available that simplified some of the functions that you intended to implement. If things were easier in some areas, then how did you adapt your project to take account of your findings?

%It is more likely that things were more complex than you first thought. In particular, were there any problems or difficulties that you found during implementation that you had to address? Did such problems simply delay you or were they more significant? Your implementation might well be described in the same chapter as Problems (see below).

\section{Basic Structure}

\subsection{Loading Data}
One of the more key parts to implement before all others in this project is the ability to load in
data from Lloyds spreadsheet. The first step of this was to convert it to a \gls{csv} file-type,
which is easier to read programatically.

The initial spreadsheet was slightly different from the version depicted below, Lloyd was kind 
enough to update it so it was easier to handle digitally. In this version of the spreadsheet the
filenames were much longer and were in sub-folders depending on their collection. Extra logic was
needed to locate the file (a simple matter of concatenating the collection to the filename as a
directory). The image files in the second version were just flat files which I decided would be
better placed in a data directory.

The move to the newer version was a good excuse to clean up some of the initial code to use better
Python programming practises; replacing messy loops with list comprehension where possible, using
\gls{kwargs} and dictionaries instead of having utility methods, etc.

From the original format show in table~\ref{tab:spreadsheet-format} 
%(see table~\ref{tab:spreadsheet} for the full document) 
the data was converted to the CSV format show
in figure~\ref{fig:csv-spreadsheet}.

\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
Filename & ID & Title & Catalogue entry BBC YP & Genre & Height & Width & Area & Materials & 
Collection & image width & image height & image height/image width \\ \hline
001.jpg & 1 & A Chapel in the Tyrol & 1950-1960 & Landscape & 40.7 & 29.8 & 1212.86 & oil on 
hardboard & NLW & 687 & 944 & 1.3741 \\ \hline
\end{tabular}
}
\caption{Layout of the Painting Data Spreadsheet}\label{tab:spreadsheet-format}
\end{table}

\begin{figure}[h]
\resizebox{\textwidth}{!}{
\texttt{001.jpg,1,A Chapel in the Tyrol,1950-1960,Landscape,40.7,29.8,1212.86,oil on hardboard,
NLW,687,944,1.3741}
}
\caption{Painting Data in CSV Format}\label{fig:csv-spreadsheet}
\end{figure}

This \gls{csv} was then parsed using the Python 2.7 in-built \texttt{csv} module with relative 
ease. The example code (see listing~\ref{lst:csv_example_code}) helped with correct resource 
management as, at this point, my Python knowledge was fairly low.


\subsection{Top-Level Classes}
Being used to Java, an initial instinct of mine was to set up interfaces for the majority of 
top-level classes (namely the \texttt{Analyser} and \texttt{Classifier} classes). However, 
because of Python's duck typing, there wasn't possibly to create these classes as interfaces. I 
decided to create them as abstract stub classes which just held placeholder method definitions 
which needed to be overridden in the concrete sub-classes. This was a slightly pointless exercise 
due to the weak typing, but useful for me to get my head around the architecture.

I also implemented the \texttt{Painting} class and had a method to take an array (from the CSV 
file) and create a new instance of it from this. Later this was changed to use Python 
\gls{kwargs} instead as it made more sense for the job. This change was done during the change to 
the second version of Lloyds database.


\subsection{Command Line Interface}
As a research program with lots of different analysis and classification techniques the next step
in setting up the basic architecture was to create a set of command-line arguments which would
switch the functionality of the program. These arguments are depicted in table~\ref{tab:args}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|} \hline
Name             & Short Flag & Long Flag         & Description\\\hline
Analyser         & \verb+-a+  & \verb+--analyser  +& Switch the analysis technique\\\hline
Machine Learning & \verb+-m+  & \verb+--ml        +& Switch the machine learning technique\\\hline
Data             & \verb+-d+  & \verb+--csv       +& The data file to use\\\hline
GUI              & \verb+-g+  & \verb+--gui       +& Switch the GUI visualisation\\\hline
Binning          & \verb+-5+  & \verb+--bin-years +& Put paintings in 5 year bins\\\hline
Export           & \verb+-e+  & \verb+--export    +& Export analysed data\\\hline
\end{tabular}
\caption{Command Line Arguments}\label{tab:args}
\end{table}

The \texttt{argparse} library handles this nicely, the example code shown in 
listing~\ref{lst:argparse_example_code} shows the usage of this library.

These flags then hook into the factory methods to actually create the instances and return them to
the central point which then called them where required.


\section{Colour Space Analysis}
Colour space analysis involves performing statistical analysis on different colour models 
(\gls{rgb}, \gls{hsv}, etc.). This gives a very simplistic view of the entire image.

OpenCV offers the \texttt{Avg} method to perform the average across the image, however with a 
further look into the documentation there is also the \texttt{AvgStd} method which performs both 
mean and standard deviation on an image.

The analysed data was just the tuple returned by the \texttt{AvgStd} method. The distance measure
was defined to be the sum of all elements in the tuple (in the case of an \gls{rgb} colour model
the mean red, green and blue and the standard deviation of red, green and blue).

\subsection{Colour Models}
There are many colour models to consider with digital image processing. \Gls{rgb} is one of the
better know colour spaces as it is often how images are captured. It does have a problem in that
all three values can change when the brightness changes.

As one of the main principals of this project is that Kyffin Williams' work darkened over time, it
should follow that \gls{rgb} may not be the best colour model to use.

To account for this it was decided to also use a \gls{hsv} colour model to compare and contrast to
\gls{rgb}.

OpenCV handles colour spaces slightly oddly. Initially it uses \texttt{LoadImageM} to load the 
image, which uses an integer argument as a flag to define whether the image should be loaded in 
colour or grayscale.

From this image you then can use \texttt{CvtColor} to convert the colour model of an image, which 
uses an integer argument as a flag to define a number of different colour spaces.

Once converted, all methods act exactly the same as they would on a \gls{rgb} image.

\subsection{Colour Histograms}
Colour Histograms are a representation of the distribution of colour across an image; this makes
them very powerful for analysing the colour space of an image.

OpenCV provides a number of methods for both calculating and operating upon colour histograms. 
Here the example code (figure~\ref{lst:opencv_histogram}) from the OpenCV documentation was a
useful reference as some of the details of creating histograms in OpenCV are not noted implicitly
in the python documentation.

The distance measure between colour histograms is also handled by OpenCV using a compare histogram
method. Under the covers this uses a Chi Squared (equation~\ref{eq:chi_squared}) method to
compare to histograms of equal dimensions.

\begin{equation}\label{eq:chi_squared}
d(H_1,H_2) =\sum_I{\frac{(H_1(I) - H_2(I))^2}{H_1(I)}}
\end{equation}


\section{Texture Analysis}

\subsection{Edge Count}
The simplest technique to analyse texture is to apply an edge detection algorithm over the image
and produce a count of all the marked edges regardless of orientation.

The Canny edge detector\cite{Canny1986Computational} provided a good output for measuring this, 
operators like Sobel provided less useful results and thus provided a worse analysis technique.

From the generated image a histogram was produced with only 2 bins; one bin for the presence of an
edge, the other for the absence of an edge.

Open CV provides methods for applying the Canny edge detector to an image so this technique was 
simple to implement.

%TODO Image of the Canny'd image.

\subsection{Histograms of Orientation Gradients}
This technique is based on the \gls{hog} paper\cite{Dalal2005Histograms} and mirrors some of the
implementation in a simplified fashion.

The main crux of the method was to generate the exact orientation of a given point of an image,
apply this across the image and then bin the results into a histogram. The original paper then
focused on using this for human recognition and aimed to keep the time complexity down through
normalisation, applying it to this project only needed the generation of the histogram of exact
orientations.

Calculating the edge orientation involves passing a filter over an image which is used to find the
orientation of that gradient. 

There are many forms of filter which can be used to do this. The simplest of which discrete 
derivative masks (figure~\ref{fig:1x2-ddm}, figure~\ref{fig:1x3-ddm}, etc.), steerable filters 
are a more adjustable implementation of this. There are also more complex filters, like Gabor 
filters, which provide better flexibility and matching.

Using discrete derivative masks it is fairly simple to work out to orientation of a gradient 
mathematically. First pass a mask of the form $\begin{bmatrix}-1\\0\\1\end{bmatrix}$, followed
by one in the form $\begin{bmatrix}-1 & 0 & 1\end{bmatrix}$. These give you 
$\frac{\delta f}{\delta x}$ and $\frac{\delta f}{\delta y}$ respectively.

From these values you can then use a gradient direction function, shown in 
equation~\ref{eq:grad_dir}, to work out the actual direction of the gradient.

\begin{equation} \label{eq:grad_dir}
\theta = {\rm atan2}\left( \frac{\delta f}{\delta x}, \frac{\delta f}{\delta y} \right)
\end{equation}

Working out the exact orientation of a gradient is a costly operation and it became useful to
\emph{can} (persist) the results to allow the technique to complete in a decent amount of time.
This change was made after the change to \emph{cv2} and was done using \emph{numpy} methods for
simplicity. 

Rather than can the resulting histograms it was actually easier to can the output gradients as 
this allowed histograms with varying bin sizes to be used; this allows a comparison between 
steerable and Gabor filters to these histograms.

\subsection{Steerable Filters}
Another way of doing this is to change the orientation of the filter and bin the direction into 
certain cardinal directions; typically $0$, $\frac{\pi}{4}$, $\frac{\pi}{2}$ and $\frac{3\pi}{4}$
(from $\pi$ to $2\pi$ becomes a repeat of any of those directions, only in reverse and are, 
therefore, covered already).

To do this we can use steerable filters $S(\theta)$ (shown in figure~\ref{fig:steerable-filters}). 
With these we adjust the angle they are designed to match ($\theta$), which will then give the 
degree to which a gradient matches that angle. To complete the binning by orientation, this value 
will then have a threshold applied, then added to the bin if the value is above the threshold.

\begin{figure}[h]
$$
S\left(0\right) = 
\begin{bmatrix}
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0
\end{bmatrix}
S\left(\frac{\pi}{4}\right) =
\begin{bmatrix}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{bmatrix}
S\left(\frac{\pi}{2}\right) = 
\begin{bmatrix}
0 & 0 & 0 \\
1 & 1 & 1 \\
0 & 0 & 0
\end{bmatrix}
S\left(\frac{3\pi}{4}\right) = 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$
\caption{Steerable Filters}
\label{fig:steerable-filters}
\end{figure}

These have the advantage of being far less complex to compute than edge orientation; the maths and
histogram creation on a floating point array are long running operations, to the point where
canning the results for edge orientation is desirable.

There is the obvious trade off that steerable filters can only work in four directions which
does affect the performance quite severely.

\subsection{Gabor Filters}
To increase the orientations matched from steerable filters without the need for increasingly 
large kernels, Gabor filters\cite{Daugman1985Uncertainty} were implemented. Gabor filters allow
for an almost-infinite range of orientations without affecting the size of the filter. Equation
\ref{eq:gabor} shows the mathematics behind Gabor Filters.


\begin{align}\label{eq:gabor}
g(x, y; \lambda, \theta, \psi, \sigma, \gamma) &= \exp \left( -\frac{x'^2+\gamma^2y'^2}{2\sigma^2} \right) \cos \left( 2\pi \frac{x'}{\lambda} + \psi \right)
\intertext{where:}
x' &= x \cos \theta + y \sin \theta \nonumber \\
y' &= x \sin \theta + y \cos \theta \nonumber
\end{align}

As with steerable filters the results of the filter were passed into a histogram. Initially Gabor
filters were implemented with the same 4 orientations as steerable filters ($0$, $\frac{\pi}{4}$,
$\frac{\pi}{2}$ and $\frac{3\pi}{4}$), however this has two problems:

\begin{enumerate}
\item The variation in orientations is not being utilised and,
\item Divide by 0 errors from python.
\end{enumerate}

To solve both of these issues I changed to way Gabor filters were initialised, adding in a 
parameter which defined the number of orientations that instance of the Gabor filter would have,
the logic of which is defined in figure~\ref{fig:gabor-init}, note how the range is $1\dots b$, 
solving the divide by 0 issue as $\theta=0$ and $\theta=\pi$ will produce the same result (both
matching vertical lines best).

\begin{figure}[h]
\begin{algorithmic}
\Function{Gabor.init}{$b$} \Comment{$b$ is the number of orientations}
\For {$i \in 1\dots b$}
\State $\theta_i \gets \frac{i\pi}{b}$ \Comment{$\theta$ is the list of orientations to produce filters for}
\EndFor
\EndFunction
\end{algorithmic}
\caption[Psuedocode for Gabor Filter initialisation]{Psuedocode for Gabor Filter initialisation including number of orientations to produce 
filters for}\label{fig:gabor-init}
\end{figure}

The actual Gabor filters are then produced for all values in $\theta$ at runtime and applied to
the images in turn.

\section{Brush-Stroke Analysis}

Due to time constraints it was not viable to look at brush-stroke analysis, especially with the
shift of focus to exemplars after texture analysis.

It would have been nice to explore this area due to Williams' style, but a lack of high-resolution
images would have also made this a difficult area to get right.

There were several good ideas thought of for this area. One was to train a machine learning 
technique to recognise brush-strokes through the manual marking of strokes. This would have been a
very interesting method to try, especially being able to apply existing techniques to aid and
improve the classification of brush-strokes.

Other techniques involved passing filters over the image looking for the edges of brush-strokes
\cite{Berezhnoy2009Automatic}, then applying other operations to complete the stroke area. This
is a much simpler technique which will likely give bad results due to the low resolution and
non-uniform scaling of the images as it seems to be very dependant on the filter.

One final technique suggested by Li et al.\cite{Li2012Rhythmic} involves a combination of edge
analysis and clustering in colour space with a number of heuristics involving branching, 
stroke-width modelling and gap filling to refine the original brush-stroke estimates.

The difficulty that would be associated with this section of work is that existing techniques may
not be applicable to Williams' work due to his use of the palette knife. Whilst there are very
obviously clear strokes in his work, comparing these strokes to the likes of Van Gogh are unlikely
to pay off due to the \emph{blockier} strokes of the palette knife. Another reason this analysis
technique was side-lined in favour of exemplars.


\section{Ensemble Methods}
Ensemble methods are commonly used in statistical and machine learning to improve the performance,
the main concept of an ensemble method is to combine two or more models to produce better results
than would be gained from a single model.

Practically, ensemble methods just involve running all models in the ensemble, combining them into
one large histogram and then perform the normal distance measure on them. However, due to some 
problems with the \emph{OpenCV cv2} histogram calculation method, it was a lot easier to flatten
(to collapse a multi-dimensional array into a one-dimensional array) the results from each 
technique have use this as the model.

This does affect the performance a little, but it is better than the other solution of adding 
resulting distance from each model together as the distances are not normalised. This was my 
initial solution to the problem and hugely affected the results as the distance returned from
statistical colour-space methods is a lot less than those returned from colour histograms and so
on.

Of course it would also be possible to normalise each distance measure of each technique, but this
would be difficult as the mean and variance of the distance for an analysis technique is not 
tracked. It may also cause problems when new examples are added (during validation, for example)
which affect both these values.

This method is commonly known as \gls{bagging}; where each model has a ``vote'' with equal weight.

One part I considered adding to this, but didn't have the time to complete, would be to weight the
vote of each technique (likely via a machine learning technique) in the ensemble so that the 
better performing techniques affect the results more.

Another method commonly used in machine learning is Boosting, where the model is built 
incrementally, with the focus being on examples which were mis-classified by the previous models.
This method does improve accuracy, but is known to over-fit the data set. 


\section{Classification and Validation}
With the results of analysis techniques there's also a need to be able to classify paintings based
on these results and also to validate the results of both so that different methods of analysis
and classification can be compared for performance.

\subsection{$k$-Nearest Neighbour}
The simplest method of classification is to work out which painting in the data set is closest in
feature space to the painting which needs to be classified and classify the year of this painting
with the year of the nearest painting.

This is effective a $k$-Nearest Neighbour with $k=1$, figure~\ref{fig:1nn} shows the psuedocode
for this algorithm

\begin{figure}[h]
\begin{algorithmic}
\Function{1-NN}{$p$, $data$}
\State $n = \operatorname*{arg\,min}_{d \in data}$ \Call{Distance}{$p, d$}
\State\Return $n_{year}$
\EndFunction
\end{algorithmic}
\caption{Psuedocode for 1-Nearest Neighbour}\label{fig:1nn}
\end{figure}

From 1-Nearest Neighbour it is a simple matter to change this to a $k$-Nearest Neighbour algorithm
as figure~\ref{fig:knn} shows.

\begin{figure}[h]
\begin{algorithmic}
\Function{k-NN}{$p$, $data$, $k$}
\ForAll {$k$}
\State $n = \operatorname*{arg\,min}_{d \in data}$ \Call{Distance}{$p, d$}
\State $a \gets a + n_{year}$
\State $data \gets data \setminus n$
\EndFor
\State \Return \Call{Average}{$a$}
\EndFunction
\end{algorithmic}
\caption{Psuedocode for $k$-Nearest Neighbour}\label{fig:knn}
\end{figure}

With this, the method of averaging the year can be any statistical method of taking the average
(mean, median or mode). Mean is the typical case for $k$-Nearest Neighbour as it is simple to
implement. Median is less common as it is slightly more complex to implement. Mode is more 
difficult to implement programmatically, especially given the sparseness of the data. 

$k$-Nearest Neighbour is good for this project as it makes no assumptions about the state of the
data, it only uses points in feature space to function. Initially, when feature space data wasn't
stored exclusively in histograms with was useful as it allowed different distance measure to be
applied without needing to change the classifier.


\subsection{Leave-One-Out Cross Validation}
To validate the classified years from $k$-Nearest Neighbour, this project uses Leave-one-out Cross
Validation; a technique which involves removing each point from the known data set, applying the
classification technique to this point against the remaining data set. This produces a list of
classified years against the actual years.

To measure the goodness of fit the Pearson's product-moment correlation coefficient was 
calculated on these orderings; this provides us with a performance measure of each classifier. 
It is also possible to test Pearson's R for statistical significance.

Pearson's correlation is perfect for this project; A perfect correlation of $1$ is achieved by
having the classified years exactly match the actual years.

Both of these are easy to implement - leave-one-out cross validation just involves iterating over
the entire list of known paintings, popping the current painting from the list and running the 
classifier on it, then pushing it back into the list at it's original position. Pearson's 
correlation is implemented by \emph{numpy} with Pearson's R for statistical significance included
as a part of the method.

As a part of this \emph{matplotlib} was used to plot this data graphically; as a scatter plot for
actual versus classified year and as a histogram for the number of years out the classifier was.

%TODO Scatter plot
%TODO histogram of years out


\subsection{Weka 3}
Weka is a collection of machine learning algorithms\cite{Hall2009WEKA}, used predominately for
data mining. Due to the number of machine learning techniques it provides it was interesting to
input data from the analysis techniques into Weka.

%TODO More on Weka


\subsubsection{Attribute-Relation File Format (ARFF)}
Part of inputting data into Weka is to produce Attribute-Relation File Format (ARFF) from the
analysis techniques. ARFF is the main format supported by Weka, so it is one of the easiest ways
of importing the data into Weka.

A few libraries exist for ARFF conversion in Python, however they all have their own quirks. 
\emph{liac-arff} was one of the easiest to use, but doesn't support the date attribute. This isn't
a problem for our current project as we only use year which can easily be represented as an 
integer instead. I did intend to contribute a working patch for dates for the library, but it 
would have taken too much time out of the project to get it working correctly.


\subsection{Exemplars}
Another method of classification is to incorporate expert knowledge within the framework. For each
year represented in the collection Dr Paul Joyner, of the National Library of Wales was asked to
choose the one painting which best represented Williams' work for that year. Dr. Joyner is a 
member of the Trustees of the Kyffin Williams Estate and he has written widely on Welsh Art and
Kyffin Williams.

These chosen paintings are considered to be connoisseurially or artistically selected exemplars
(\emph{artistic exemplars}, for short), which can be used as a representation of that particular
year.


\subsubsection{Nearest Exemplar Classification}
The most obvious use for artistic exemplars is to classify a given example using the nearest 
artistic exemplar, rather than other members of the data set, as depicted in 
figure~\ref{fig:nearest-exemplar}.

\begin{figure}[h]
\begin{algorithmic}
\Function{NearestExemplar}{$p$}
\State $n \gets \operatorname*{arg\,min}_{e \in exemplars}$ \Call{Distance}{$p$,$e$}
\State \Return $n_{year}$
\EndFunction
\end{algorithmic}
\caption{Psuedocode for Nearest Exemplar Classification}\label{fig:nearest-exemplar}
\end{figure}

To implement Nearest Exemplar Classification was a fairly easy task: a secondary spreadsheet was 
provided which contained all the necessary information of exemplar by year (see 
table~\ref{tab:exemplar-spreadsheet} for the full document).

The spreadsheet was arranged in the format described in table~\ref{tab:exemplar-layout}, from
there it was a simple matter of saving the spreadsheet as a \gls{csv} file and taking some of the
existing code for parsing \gls{csv} files. This caused a slight problem in that the parsed data
didn't have enough information to create a full \texttt{Painting} object, yet all the analysis
techniques worked from these objects.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline
Filename & ID  & Title                      & Catalogue Entry & Year \\\hline
154.jpg  & 154 & Landscape at Llanaelhaearn & 1947            & 1947 \\\hline
\multicolumn{5}{|c|}{\textit{etc.}}\\\hline
\end{tabular}
\caption{Layout of the Exemplar Spreadsheet}\label{tab:exemplar-layout}
\end{table}

This was solved easily thanks to Python's dynamic typing. A simple class which implemented all the
necessary elements of \texttt{Painting} could be passed to the analysis techniques without any
complaints. With a statically typed language this would have been harder to complete, but there
would have been ways around using sub-classes and so on.

With the exemplars loaded and analysed, the program could continue as normal, until the
classification step.

The idea of Nearest Exemplar Classification is to classify the unknown example using the nearest
exemplar to that example in the feature space. This acts as a $k$-Nearest Neighbour with $k=1$ and
the space of neighbours only including the exemplars, rather than every other example. The 
psuedocode for this is shown in figure~\ref{fig:nec-psuedo}.

Initially this was implemented so that the examples that were exemplars were also classified, but
this is a pointless exercise which only skews the results. Additional logic to remove any exemplar
which matched the current example.

\begin{figure}[h]
\begin{algorithmic}
\Function{NearestExemplarClassification}{$p$}
\State $n \gets \operatorname*{arg\,min}_{e \in exemplars \setminus p}$ \Call{Distance}{$p$, $e$}
\State \Return $n_{year}$
\EndFunction
\end{algorithmic}
\caption[Psuedocode for Nearest Exemplar Classification with Added Logic]{Psuedocode for Nearest Exemplar Classification with Logic to Remove $p$ from $exemplars$}\label{fig:nec-psuedo}
\end{figure}

This proved to give slightly worse correlation per technique than $k$-Nearest Neighbour. This 
result is to be expected; for a start an artistically classified exemplar is unlikely to be the
same as a statistically classified exemplar (see section~\ref{sec:sce}). Also, with fewer examples
to classify against, any variance in the data set (of which there is a lot) will likely be 
magnified.

Lastly, a painting may be picked as an exemplar by an expert for different reasons than any
analysis technique that currently exists can give; emotional connections and knowledge of the
artists history can be very subjective and may not relate to anything put down in paint.


\subsection{Statistically Classified Exemplars}\label{sec:sce}

Another approach to exemplars is to work out a theoretical exemplar for a given period; the 
centroid of paintings within the given feature space for a single year, for example.

The simplest way of working this out is showing in figure~\ref{fig:sce-psuedo}, this works by 
taking the mean (equation~\ref{eq:mean}) of each feature in the set of paintings for a single 
year, this will give the point in feature space that is most central. This is the same technique
used to generate centroids in a clustering algorithm ($k$-Means Clustering, for example).

\begin{figure}[h]
\begin{algorithmic}
\Function{StatisticalClassifyExemplar}{$examples$}
\ForAll{$example \in examples$}
\ForAll{$feature \in example_{features}$}
\State $average_{feature} \gets average_{feature} + example_{feature}$
\EndFor
\EndFor
\ForAll{$feature \in average$}
\State$average_{feature} \gets \frac{average_{feature}}{\Call{Length}{examples}}$
\EndFor
\State \Return $average$
\EndFunction
\end{algorithmic}
\caption{Psuedocode for generating a Statistically Classified Exemplar}\label{fig:sce-psuedo}
\end{figure}

This may become a long operation depending on how many dimensions the feature space has. A
technique like \gls{pca} may be useful to help cut down the number of dimensions needed that this
algorithm uses.

Initially, as I wasn't producing histogram results from analysis techniques, this was a little
complex to do; each analysis technique had to have its own method of generating the centroid and
returning the data for this centroid. The change to histogram results made everything a lot easier
and, combined with the \texttt{mean} method from \emph{numpy}, which can handle the mean along 
different axes, rather than just over a flattened array. This is the perfect too for the job as
histograms are returned as numpy arrays and the data passed into the centroid method is now an 
array of these histograms. Calculating the mean across the first axis has the effect of 
calculating the centroid. Equation~\ref{eq:mean-matricies} shows this mathematically on 2D 
Matrices where $\bar{x}_n$ is the mathematical mean of $x$ along axis $n$.

\begin{align}
x=&\begin{bmatrix}
\begin{bmatrix}
1.0 & 1.0 & 0.0 \\
0.0 & 1.0 & 1.0 \\
0.0 & 0.4 & 0.0 \\
\end{bmatrix} & 
\begin{bmatrix}
0.0 & 1.0 & 0.0 \\
1.0 & 1.0 & 1.0 \\
0.6 & 0.0 & 1.0 \\
\end{bmatrix}
\end{bmatrix} \nonumber \\
\bar{x}_1 =& 
\begin{bmatrix}
0.5 & 1.0 & 0.0 \\
0.5 & 1.0 & 1.0 \\
0.3 & 0.2 & 0.5 \\
\end{bmatrix}
\label{eq:mean-matricies}
\end{align}

Whilst this is a useful theoretical exemplar, it doesn't fit with the idea of an artistic 
exemplar, it is quite possible that the art historians would be able to imagine a theoretical
painting which suits that year but they were limited to real paintings so it should follow that
the statistical exemplars can also be constrained in the same fashion.

This could be seen as a form of generalisation. Though this may affect the results for the worse
it may help prevent overfitting of the classification technique. It also shows which years are 
commonly chosen as exemplars correctly and incorrectly, this should give a vision of the 
difference between the artistic and statistic exemplars and where the artistic exemplars are the
outliers of that particular year.

To do this a 1-Nearest Neighbour algorithm is applied to the data for that year from the centroid,
this, of course, has the effect of picking the painting closest to the centroid, which is then
used as the exemplar for that year. The same technique used to classify using artistic exemplars
is then applied, only with these exemplars rather than the ones loaded from the exemplar file.

This then opened up the option to use inheritance to cut down on code repetition. The artistic
exemplar classifier could be extended by the centroid classifier, with a single method changed to
generate the centroids instead of reading them from a file. This, in turn, could then be extended
by the statistical exemplar classified (as described above) so that the 1-Nearest Neighbour
algorithm could be applied at the end of the generation of exemplars. Here the use of super calls
in Python gave me a little bit of trouble, listing~\ref{lst:py-super} shows one correct solution
to this\footnote{There are, of course, other correct methods, but the one listed is one of the 
better solutions}.

\begin{lstlisting}[language=python, breaklines=true, label=lst:py-super, 
caption={Super method calls in Python}, frame=single]
# Note Base has to extend 'object' for super calls to work
class Base(object):
    def call(self, params):
        # ...

class Sub(Base):
    def call(self, params):
        # Before Base.call
        super(self, Sub).call(params)
        # After Base.call
\end{lstlisting}


\subsection{cv2 and Histogram Results}
Whilst reading through the 
\emph{numpy} and \emph{OpenCV} 
it turned out that, until this point, I had been using a deprecated version of OpenCV. \emph{cv2} 
is the new version of \emph{OpenCV} for python, which provides bindings for \emph{OpenCV 2.4} and 
uses \emph{numpy} arrays and multi-dimensional arrays to represent all data types rather than 
in-built data structures.

This makes everything a lot easier to work with as all \emph{numpy} arrays (with the correct data type)
can be passed into the \emph{cv2} methods.

