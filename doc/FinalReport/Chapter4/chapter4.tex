\chapter{Implementation}

%The implementation should look at any issues you encountered as you tried to implement your design. During the work, you might have found that elements of your design were unnecessary or overly complex, perhaps third party libraries were available that simplified some of the functions that you intended to implement. If things were easier in some areas, then how did you adapt your project to take account of your findings?

%It is more likely that things were more complex than you first thought. In particular, were there any problems or difficulties that you found during implementation that you had to address? Did such problems simply delay you or were they more significant? Your implementation might well be described in the same chapter as Problems (see below).

\section{Colour Space Analysis}
Colour space analysis involves performing statistical analysis on different colour models 
(\gls{rgb}, \gls{hsv}, etc.). This gives a very simplistic view of the entire image.

OpenCV offers the \texttt{Avg} method to perform the average across the image, however with a 
further look into the documentation there is also the \texttt{AvgStd} method which performs both 
mean and standard deviation on an image.

The analysed data was just the tuple returned by the \texttt{AvgStd} method. The distance measure
was defined to be the sum of all elements in the tuple (in the case of an \gls{rgb} colour model
the mean red, green and blue and the standard deviation of red, green and blue).

\subsection{Colour Models}
There are many colour models to consider with digital image processing. \Gls{rgb} is one of the
better know colour spaces as it is often how images are captured. It does have a problem in that
all three values can change when the brightness changes.

As one of the main principals of this project is that Kyffin Williams' work darkened over time, it
should follow that \gls{rgb} may not be the best colour model to use.

To account for this it was decided to also use a \gls{hsv} colour model to compare and contrast to
\gls{rgb}.

OpenCV handles colour spaces slightly oddly. Initially it uses \texttt{LoadImageM} to load the 
image, which uses an integer argument as a flag to define whether the image should be loaded in 
colour or grayscale.

From this image you then can use \texttt{CvtColor} to convert the colour model of an image, which 
uses an integer argument as a flag to define a number of different colour spaces.

Once converted, all methods act exactly the same as they would on a \gls{rgb} image.

\subsection{Colour Histograms}


\section{Texture Analysis}

\subsection{Edge Orientation}
\subsubsection{Histogram of Edge Orientation}


\section{Brush-Stroke Analysis}


\section{Classification and Validation}

\subsection{$k$-Nearest Neighbour}

\subsection{Leave-One-Out Cross Validation}

\subsection{Weka 3}
\subsubsection{Attribute-Relation File Format (ARFF)}

\subsection{Exemplars}
\subsubsection{Nearest Exemplar Classification}
To implement Nearest Exemplar Classification was a fairly easy task: Llyod (with help from members
of the \gls{nlw}) provided a secondary spreadsheet which contained all the necessary information 
of exemplar by year (see table~\ref{tab:exemplar-spreadsheet} for the full document).

The spreadsheet was arranged in the format described in table~\ref{tab:exemplar-layout}, from
there it was a simple matter of saving the spreadsheet as a \gls{csv} file and taking some of the
existing code for parsing \gls{csv} files. This caused a slight problem in that the parsed data
didn't have enough information to create a full \texttt{Painting} object, yet all the analysis
techniques worked from these objects.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline
Filename & ID  & Title                      & Catalogue Entry & Year \\\hline
154.jpg  & 154 & Landscape at Llanaelhaearn & 1947            & 1947 \\\hline
\multicolumn{5}{|c|}{\textit{etc.}}\\\hline
\end{tabular}
\caption{Layout of the Exemplar Spreadsheet}\label{tab:exemplar-layout}
\end{table}

This was solved easily thanks to Python's dynamic typing. A simple class which implemented all the
necessary elements of \texttt{Painting} could be passed to the analysis techniques without any
complaints. With a statically typed language this would have been harder to complete, but there
would have been ways around using sub-classes and so on.

With the exemplars loaded and analysed, the program could continue as normal, until the
classification step.

The idea of Nearest Exemplar Classification is to classify the unknown example using the nearest
exemplar to that example in the feature space. This acts as a $k$-Nearest Neighbour with $k=1$ and
the space of neighbours only including the exemplars, rather than every other example. The 
psuedocode for this is shown in figure~\ref{fig:nec-psuedo}.

Initially this was implemented so that the examples that were exemplars were also classified, but
this is a pointless exercise which only skews the results. Additional logic was added to skip any
example which was an exemplar itself.

\begin{figure}[h]
\begin{algorithmic}
\Function{NearestExemplarClassification}{$example$, $exemplars$}
\If{$example \in exemplars$}
\State \Return $\varnothing$
\EndIf
\State \Return \Call{Nearest}{$example, exemplars$}
\EndFunction
\end{algorithmic}
\caption{Nearest Exemplar Classification Psuedocode}\label{fig:nec-psuedo}
\end{figure}

This proved to give slightly worse correlation per technique than $k$-Nearest Neighbour. This 
result is to be expected; for a start an artistically classified exemplar is unlikely to be the
same as a statistically classified exemplar (see section~\ref{sec:sce}). Also, with fewer examples
to classify against, any variance in the data set (of which there is a lot) will likely be 
magnified.

Lastly, a painting may be picked as an exemplar by an expert for different reasons than any
analysis technique that currently exists can give; emotional connections and knowledge of the
artists history can be very subjective and may not relate to anything put down in paint.


\subsection{Statistically Classified Exemplars}\label{sec:sce}

Another approach to exemplars is to work out a theoretical exemplar for a given period; the 
centroid of paintings within the given feature space for a single year, for example.

The simplest way of working this out is showing in figure~\ref{fig:sce-psuedo}, this works by 
taking the mean (equation~\ref{eq:mean}) of each feature in the set of paintings for a single 
year, this will give the point in feature space that is most central. This is the same technique
used to generate centroids in a clustering algorithm ($k$-Means Clustering, for example).

\begin{figure}[h]
\begin{algorithmic}
\Function{StatisticalClassifyExemplar}{$examples$}
\ForAll{$example \in examples$}
\ForAll{$feature \in example_{features}$}
\State $average_{feature} \gets average_{feature} + example_{feature}$
\EndFor
\EndFor
\ForAll{$feature \in average$}
\State$average_{feature} \gets \frac{average_{feature}}{\Call{Length}{examples}}$
\EndFor
\State \Return $average$
\EndFunction
\end{algorithmic}
\caption{Psuedocode for generating a Statistically Classified Exemplar}\label{fig:sce-psuedo}
\end{figure}

This may become a long operation depending on how many dimensions the feature space has. A
technique like \gls{pca} may be useful to help cut down the number of dimensions needed that this
algorithm uses.


\section{3\textsuperscript{rd} Party Libraries and Tools}

\subsection{Python}
\subsubsection{Python setuptools}

\subsection{OpenCV}

\subsection{scipy \& numpy}

\subsection{matplotlib}

\subsection{Weka 3}
\subsubsection{liac-arff}

\subsection{git \& github}


